{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.13.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "import time\n",
    "\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour convertir le temp du format sec* à hh:mm:ss\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Marche_Train.shape:  (325, 40, 3, 1) Labels Marche:  (325,)\n",
      "Features Course_Train.shape:  (142, 40, 3, 1) Labels Course:  (142,)\n",
      "Features Arriere_Train.shape:  (117, 40, 3, 1) Labels Arriere:  (117,)\n",
      "\n",
      "Data_Marche_Test.shape (3360, 3)\n",
      "Data_Marche_Test.shape (3360, 3)\n",
      "Data_Arriere_Test.shape:  (1240, 3)\n",
      "\n",
      "Labels_Marche_Test.shape (84,)\n",
      "Labels_Course_Test.shape (54,)\n",
      "Labels_Arriere_Test.shape (31,)\n",
      "\n",
      "Features Test Marche.shape:  (3360, 3) Labels Test Marche.shape:  (84,)\n",
      "Features Test Course.shape:  (2160, 3) Labels Test Course.shape:  (54,)\n",
      "Features TestArriere.shape:  (1240, 3) Labels Test Arriere.shape:  (31,)\n",
      "\n",
      "Features et Labels Train:  (584, 40, 3, 1) (584,)\n",
      "\n",
      "labels_train\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "\n",
      "Features et Labels Test:  (169, 40, 3, 1) (169,)\n",
      "\n",
      "labels_test\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2.]\n"
     ]
    }
   ],
   "source": [
    "Taille_Sample = 40\n",
    "\n",
    "# Load training and eval data\n",
    "Data_Marche_Train = np.loadtxt(\"Marche_Train.txt\") \n",
    "Data_Course_Train = np.loadtxt(\"Course_Train.txt\") \n",
    "Data_Arriere_Train = np.loadtxt(\"Arriere_Train.txt\")\n",
    "\n",
    "Labels_Marche_Train = np.zeros(int(Data_Marche_Train.shape[0]/40),)     #label \"0\"pour la marche de taille du tenseur\n",
    "Labels_Course_Train = np.ones(int(Data_Course_Train.shape[0]/40),)      #label \"1\"pour la course\n",
    "Labels_Arriere_Train = np.ones(int(Data_Arriere_Train.shape[0]/40),)*2  #label \"2\"pour le train\n",
    "\n",
    "print (\"Features Marche_Train.shape: \",Data_Marche_Train.reshape(int(Data_Marche_Train.shape[0]/40),40,3, 1).shape, \"Labels Marche: \",Labels_Marche_Train.shape)\n",
    "print (\"Features Course_Train.shape: \",Data_Course_Train.reshape(int(Data_Course_Train.shape[0]/40),40,3, 1).shape, \"Labels Course: \",Labels_Course_Train.shape)\n",
    "print (\"Features Arriere_Train.shape: \",Data_Arriere_Train.reshape(int(Data_Arriere_Train.shape[0]/40),40,3, 1).shape, \"Labels Arriere: \",Labels_Arriere_Train.shape)\n",
    "\n",
    "#--------------------------------------------------------------------------#\n",
    "\n",
    "Data_Marche_Test = np.loadtxt(\"Marche_Test.txt\") \n",
    "print (\"\\nData_Marche_Test.shape\", Data_Marche_Test.shape)\n",
    "Data_Course_Test = np.loadtxt(\"Course_Test.txt\") \n",
    "print (\"Data_Marche_Test.shape\", Data_Marche_Test.shape)\n",
    "Data_Arriere_Test = np.loadtxt(\"Arriere_Test.txt\") \n",
    "print (\"Data_Arriere_Test.shape: \",Data_Arriere_Test.shape)\n",
    "\n",
    "Labels_Marche_Test = np.zeros(int(Data_Marche_Test.shape[0]/40),)     #label \"0\"pour la marche\n",
    "print (\"\\nLabels_Marche_Test.shape\", Labels_Marche_Test.shape)\n",
    "Labels_Course_Test = np.ones(int(Data_Course_Test.shape[0]/40),)      #label \"1\"pour la course\n",
    "print (\"Labels_Course_Test.shape\", Labels_Course_Test.shape)\n",
    "Labels_Arriere_Test = np.ones(int(Data_Arriere_Test.shape[0]/40),)*2  #label \"2\"pour le train\n",
    "print (\"Labels_Arriere_Test.shape\", Labels_Arriere_Test.shape)\n",
    "\n",
    "print (\"\\nFeatures Test Marche.shape: \",Data_Marche_Test.shape, \"Labels Test Marche.shape: \",Labels_Marche_Test.shape)\n",
    "print (\"Features Test Course.shape: \",Data_Course_Test.shape, \"Labels Test Course.shape: \",Labels_Course_Test.shape)\n",
    "print (\"Features TestArriere.shape: \",Data_Arriere_Test.shape, \"Labels Test Arriere.shape: \",Labels_Arriere_Test.shape)\n",
    "\n",
    "#----------------------------------------------------------------------------------#\n",
    "#ajouter à la suite de \"data_Marche_train\" les valeurs de \"data_course_train\" ensuite les valeurs de \"Data_Arriere_train\"\n",
    "#le même traitement est également fait sur les labels\n",
    "features = np.append(Data_Marche_Train,Data_Course_Train, axis=0)  \n",
    "features = np.append(features,Data_Arriere_Train, axis=0)\n",
    "labels = np.append(Labels_Marche_Train,Labels_Course_Train, axis=0)\n",
    "labels = np.append(labels,Labels_Arriere_Train, axis=0)\n",
    "features = features.reshape(int(features.shape[0]/40),40,3, 1)\n",
    "print(\"\\nFeatures et Labels Train: \",features.shape, labels.shape)\n",
    "\n",
    "print(\"\\nlabels_train\")\n",
    "print(labels)\n",
    "\n",
    "features_test = np.append(Data_Marche_Test,Data_Course_Test, axis=0)\n",
    "features_test = np.append(features_test,Data_Arriere_Test, axis=0)\n",
    "labels_test = np.append(Labels_Marche_Test,Labels_Course_Test, axis=0)\n",
    "labels_test = np.append(labels_test,Labels_Arriere_Test, axis=0)\n",
    "features_test = features_test.reshape(int(features_test.shape[0]/40),40,3, 1)\n",
    "print(\"\\nFeatures et Labels Test: \",features_test.shape, labels_test.shape)\n",
    "\n",
    "print(\"\\nlabels_test\")\n",
    "print(labels_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range (features.shape[0]):\\n    Pas = features[i,:]\\n\\n    fig, axes = plt.subplots(1)\\n    if (labels[i]==0): \\n        fig.suptitle(\\'Marche\\')\\n    elif(labels[i]==1): \\n        fig.suptitle(\\'Course\\')\\n    elif(labels[i]==2): \\n        fig.suptitle(\\'Arrière\\')\\n    axes.set_ylabel(\"Valeur accéléromètre\", fontsize=14)\\n    axes.set_xlabel(\"Sample\", fontsize=14)\\n    axes.plot(Pas[:,0])\\n    axes.plot(Pas[:,1])\\n    axes.plot(Pas[:,2])\\n    plt.show()\\n    \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in range (features.shape[0]):\n",
    "    Pas = features[i,:]\n",
    "\n",
    "    fig, axes = plt.subplots(1)\n",
    "    if (labels[i]==0): \n",
    "        fig.suptitle('Marche')\n",
    "    elif(labels[i]==1): \n",
    "        fig.suptitle('Course')\n",
    "    elif(labels[i]==2): \n",
    "        fig.suptitle('Arrière')\n",
    "    axes.set_ylabel(\"Valeur accéléromètre\", fontsize=14)\n",
    "    axes.set_xlabel(\"Sample\", fontsize=14)\n",
    "    axes.plot(Pas[:,0])\n",
    "    axes.plot(Pas[:,1])\n",
    "    axes.plot(Pas[:,2])\n",
    "    plt.show()\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * __https://www.tensorflow.org/tutorials/estimators/cnn__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-414349fae5f3>:7: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/sterling/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      " Epoch : 0  Loss:  1.4523311 Test_Loss:  1.5243303\n",
      " Accuracy:  0.19178082 Accuracy Test:  0.17159763\n",
      " Epoch : 1  Loss:  1.1526196 Test_Loss:  1.3648564\n",
      " Accuracy:  0.5565069 Accuracy Test:  0.49704143\n",
      " Epoch : 2  Loss:  2.6968358 Test_Loss:  2.518256\n",
      " Accuracy:  0.32363012 Accuracy Test:  0.34319526\n",
      " Epoch : 3  Loss:  1.5122408 Test_Loss:  1.5084952\n",
      " Accuracy:  0.24486302 Accuracy Test:  0.3195266\n",
      " Epoch : 4  Loss:  1.0846418 Test_Loss:  1.1011626\n",
      " Accuracy:  0.5650685 Accuracy Test:  0.52662724\n",
      " Epoch : 5  Loss:  0.9907683 Test_Loss:  1.0074492\n",
      " Accuracy:  0.60102737 Accuracy Test:  0.556213\n",
      " Epoch : 6  Loss:  0.90714234 Test_Loss:  0.9315799\n",
      " Accuracy:  0.6626712 Accuracy Test:  0.62721896\n",
      " Epoch : 7  Loss:  0.81909597 Test_Loss:  0.8542289\n",
      " Accuracy:  0.7020548 Accuracy Test:  0.6745562\n",
      " Epoch : 8  Loss:  0.7385651 Test_Loss:  0.7826866\n",
      " Accuracy:  0.72602737 Accuracy Test:  0.70414203\n",
      " Epoch : 9  Loss:  0.666455 Test_Loss:  0.7121588\n",
      " Accuracy:  0.75 Accuracy Test:  0.7337278\n",
      " Epoch : 10  Loss:  0.604572 Test_Loss:  0.6706058\n",
      " Accuracy:  0.7482877 Accuracy Test:  0.69822484\n",
      " Epoch : 11  Loss:  0.60260344 Test_Loss:  0.63879967\n",
      " Accuracy:  0.7619863 Accuracy Test:  0.74556214\n",
      " Epoch : 12  Loss:  1.2143866 Test_Loss:  1.6012553\n",
      " Accuracy:  0.56164384 Accuracy Test:  0.49704143\n",
      " Epoch : 13  Loss:  1.4141821 Test_Loss:  1.3654109\n",
      " Accuracy:  0.24315068 Accuracy Test:  0.3195266\n",
      " Epoch : 14  Loss:  1.0133027 Test_Loss:  1.02559\n",
      " Accuracy:  0.65924656 Accuracy Test:  0.591716\n",
      " Epoch : 15  Loss:  0.875493 Test_Loss:  0.8826357\n",
      " Accuracy:  0.7123288 Accuracy Test:  0.71005917\n",
      " Epoch : 16  Loss:  0.8038607 Test_Loss:  0.829868\n",
      " Accuracy:  0.74486303 Accuracy Test:  0.7337278\n",
      " Epoch : 17  Loss:  0.73288345 Test_Loss:  0.78339607\n",
      " Accuracy:  0.74657536 Accuracy Test:  0.7218935\n",
      " Epoch : 18  Loss:  0.66862005 Test_Loss:  0.7409934\n",
      " Accuracy:  0.74486303 Accuracy Test:  0.71597636\n",
      " Epoch : 19  Loss:  0.612438 Test_Loss:  0.7002179\n",
      " Accuracy:  0.7585617 Accuracy Test:  0.71597636\n",
      " Epoch : 20  Loss:  0.5594609 Test_Loss:  0.65785295\n",
      " Accuracy:  0.7876712 Accuracy Test:  0.7278106\n",
      " Epoch : 21  Loss:  0.5098305 Test_Loss:  0.6158218\n",
      " Accuracy:  0.8099315 Accuracy Test:  0.75739646\n",
      " Epoch : 22  Loss:  0.46400875 Test_Loss:  0.57371956\n",
      " Accuracy:  0.87157536 Accuracy Test:  0.7928994\n",
      " Epoch : 23  Loss:  0.4272384 Test_Loss:  0.5399141\n",
      " Accuracy:  0.8339041 Accuracy Test:  0.78698224\n",
      " Epoch : 24  Loss:  0.4150904 Test_Loss:  0.5276958\n",
      " Accuracy:  0.91952056 Accuracy Test:  0.84023666\n",
      " Epoch : 25  Loss:  0.502311 Test_Loss:  0.6506828\n",
      " Accuracy:  0.7619863 Accuracy Test:  0.7278106\n",
      " Epoch : 26  Loss:  0.669098 Test_Loss:  0.79398674\n",
      " Accuracy:  0.7311644 Accuracy Test:  0.6035503\n",
      " Epoch : 27  Loss:  0.8859909 Test_Loss:  1.0926076\n",
      " Accuracy:  0.7328767 Accuracy Test:  0.6745562\n",
      " Epoch : 28  Loss:  0.5550382 Test_Loss:  0.58988225\n",
      " Accuracy:  0.760274 Accuracy Test:  0.70414203\n",
      " Epoch : 29  Loss:  0.45913112 Test_Loss:  0.5725288\n",
      " Accuracy:  0.7773973 Accuracy Test:  0.7218935\n",
      " Epoch : 30  Loss:  0.40146428 Test_Loss:  0.43924224\n",
      " Accuracy:  0.8681507 Accuracy Test:  0.84615386\n",
      " Epoch : 31  Loss:  0.35940906 Test_Loss:  0.45524552\n",
      " Accuracy:  0.86130136 Accuracy Test:  0.82840234\n",
      " Epoch : 32  Loss:  0.33076036 Test_Loss:  0.3950633\n",
      " Accuracy:  0.92808217 Accuracy Test:  0.8579882\n",
      " Epoch : 33  Loss:  0.30775195 Test_Loss:  0.4165687\n",
      " Accuracy:  0.91952056 Accuracy Test:  0.84615386\n",
      " Epoch : 34  Loss:  0.2895439 Test_Loss:  0.3657458\n",
      " Accuracy:  0.9434931 Accuracy Test:  0.88757396\n",
      " Epoch : 35  Loss:  0.27438018 Test_Loss:  0.40235102\n",
      " Accuracy:  0.9349315 Accuracy Test:  0.84615386\n",
      " Epoch : 36  Loss:  0.26285958 Test_Loss:  0.34010208\n",
      " Accuracy:  0.94520545 Accuracy Test:  0.9053255\n",
      " Epoch : 37  Loss:  0.25635836 Test_Loss:  0.42196772\n",
      " Accuracy:  0.9417808 Accuracy Test:  0.84615386\n",
      " Epoch : 38  Loss:  0.26234046 Test_Loss:  0.33050025\n",
      " Accuracy:  0.9400685 Accuracy Test:  0.8994083\n",
      " Epoch : 39  Loss:  0.28116396 Test_Loss:  0.5282703\n",
      " Accuracy:  0.91780823 Accuracy Test:  0.8106509\n",
      " Epoch : 40  Loss:  0.36736155 Test_Loss:  0.4582252\n",
      " Accuracy:  0.87842464 Accuracy Test:  0.80473375\n",
      " Epoch : 41  Loss:  0.47516853 Test_Loss:  0.8756435\n",
      " Accuracy:  0.8184931 Accuracy Test:  0.7218935\n",
      " Epoch : 42  Loss:  0.63929886 Test_Loss:  0.7944888\n",
      " Accuracy:  0.75 Accuracy Test:  0.6035503\n",
      " Epoch : 43  Loss:  0.3848198 Test_Loss:  0.6875256\n",
      " Accuracy:  0.8681507 Accuracy Test:  0.7751479\n",
      " Epoch : 44  Loss:  0.23868561 Test_Loss:  0.344902\n",
      " Accuracy:  0.95719177 Accuracy Test:  0.88757396\n",
      " Epoch : 45  Loss:  0.21408068 Test_Loss:  0.3329455\n",
      " Accuracy:  0.95547944 Accuracy Test:  0.88757396\n",
      " Epoch : 46  Loss:  0.19715767 Test_Loss:  0.31256378\n",
      " Accuracy:  0.9520548 Accuracy Test:  0.9112426\n",
      " Epoch : 47  Loss:  0.18406056 Test_Loss:  0.3075027\n",
      " Accuracy:  0.95719177 Accuracy Test:  0.8994083\n",
      " Epoch : 48  Loss:  0.17296746 Test_Loss:  0.2963276\n",
      " Accuracy:  0.95547944 Accuracy Test:  0.91715974\n",
      " Epoch : 49  Loss:  0.16367851 Test_Loss:  0.29191148\n",
      " Accuracy:  0.9606164 Accuracy Test:  0.9112426\n",
      " Epoch : 50  Loss:  0.15565313 Test_Loss:  0.28500107\n",
      " Accuracy:  0.9606164 Accuracy Test:  0.9112426\n",
      " Epoch : 51  Loss:  0.14862342 Test_Loss:  0.28373462\n",
      " Accuracy:  0.9623288 Accuracy Test:  0.9112426\n",
      " Epoch : 52  Loss:  0.14207524 Test_Loss:  0.27751908\n",
      " Accuracy:  0.9640411 Accuracy Test:  0.9112426\n",
      " Epoch : 53  Loss:  0.1365028 Test_Loss:  0.27561685\n",
      " Accuracy:  0.96575344 Accuracy Test:  0.91715974\n",
      " Epoch : 54  Loss:  0.13131084 Test_Loss:  0.27298462\n",
      " Accuracy:  0.96746576 Accuracy Test:  0.9112426\n",
      " Epoch : 55  Loss:  0.12748437 Test_Loss:  0.27339202\n",
      " Accuracy:  0.9691781 Accuracy Test:  0.91715974\n",
      " Epoch : 56  Loss:  0.12372159 Test_Loss:  0.26848847\n",
      " Accuracy:  0.97431505 Accuracy Test:  0.9112426\n",
      " Epoch : 57  Loss:  0.12207104 Test_Loss:  0.2751512\n",
      " Accuracy:  0.9691781 Accuracy Test:  0.9053255\n",
      " Epoch : 58  Loss:  0.120083146 Test_Loss:  0.27385753\n",
      " Accuracy:  0.96575344 Accuracy Test:  0.8994083\n",
      " Epoch : 59  Loss:  0.12309069 Test_Loss:  0.28037602\n",
      " Accuracy:  0.9708904 Accuracy Test:  0.8994083\n",
      " Epoch : 60  Loss:  0.12029313 Test_Loss:  0.28485128\n",
      " Accuracy:  0.9640411 Accuracy Test:  0.8816568\n",
      " Epoch : 61  Loss:  0.13016643 Test_Loss:  0.29753098\n",
      " Accuracy:  0.9640411 Accuracy Test:  0.9053255\n",
      " Epoch : 62  Loss:  0.123496056 Test_Loss:  0.2955414\n",
      " Accuracy:  0.9589041 Accuracy Test:  0.88757396\n",
      " Epoch : 63  Loss:  0.1362555 Test_Loss:  0.31659663\n",
      " Accuracy:  0.9606164 Accuracy Test:  0.88757396\n",
      " Epoch : 64  Loss:  0.119084395 Test_Loss:  0.28699237\n",
      " Accuracy:  0.9606164 Accuracy Test:  0.88757396\n",
      " Epoch : 65  Loss:  0.12032173 Test_Loss:  0.3035782\n",
      " Accuracy:  0.9640411 Accuracy Test:  0.9053255\n",
      " Epoch : 66  Loss:  0.09798981 Test_Loss:  0.2538743\n",
      " Accuracy:  0.9726027 Accuracy Test:  0.8994083\n",
      " Epoch : 67  Loss:  0.09013011 Test_Loss:  0.2671918\n",
      " Accuracy:  0.9726027 Accuracy Test:  0.9112426\n",
      " Epoch : 68  Loss:  0.081132054 Test_Loss:  0.2366117\n",
      " Accuracy:  0.9828767 Accuracy Test:  0.91715974\n",
      " Epoch : 69  Loss:  0.07713532 Test_Loss:  0.25356254\n",
      " Accuracy:  0.9811644 Accuracy Test:  0.91715974\n",
      " Epoch : 70  Loss:  0.07396547 Test_Loss:  0.23206224\n",
      " Accuracy:  0.9828767 Accuracy Test:  0.91715974\n",
      " Epoch : 71  Loss:  0.07142468 Test_Loss:  0.25300416\n",
      " Accuracy:  0.98458904 Accuracy Test:  0.9112426\n",
      " Epoch : 72  Loss:  0.069261886 Test_Loss:  0.2297646\n",
      " Accuracy:  0.98458904 Accuracy Test:  0.91715974\n",
      " Epoch : 73  Loss:  0.067148335 Test_Loss:  0.2557409\n",
      " Accuracy:  0.98630136 Accuracy Test:  0.9112426\n",
      " Epoch : 74  Loss:  0.06532715 Test_Loss:  0.22721097\n",
      " Accuracy:  0.98630136 Accuracy Test:  0.92899406\n",
      " Epoch : 75  Loss:  0.063437775 Test_Loss:  0.259221\n",
      " Accuracy:  0.9880137 Accuracy Test:  0.9112426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch : 76  Loss:  0.06197957 Test_Loss:  0.22415522\n",
      " Accuracy:  0.98630136 Accuracy Test:  0.93491125\n",
      " Epoch : 77  Loss:  0.0601968 Test_Loss:  0.26322752\n",
      " Accuracy:  0.989726 Accuracy Test:  0.9112426\n",
      " Epoch : 78  Loss:  0.058966618 Test_Loss:  0.22168495\n",
      " Accuracy:  0.9880137 Accuracy Test:  0.92899406\n",
      " Epoch : 79  Loss:  0.05712998 Test_Loss:  0.26659873\n",
      " Accuracy:  0.989726 Accuracy Test:  0.9112426\n",
      " Epoch : 80  Loss:  0.055971954 Test_Loss:  0.22192462\n",
      " Accuracy:  0.9880137 Accuracy Test:  0.92899406\n",
      " Epoch : 81  Loss:  0.05414154 Test_Loss:  0.26689807\n",
      " Accuracy:  0.9914383 Accuracy Test:  0.9112426\n",
      " Epoch : 82  Loss:  0.05285265 Test_Loss:  0.22528388\n",
      " Accuracy:  0.989726 Accuracy Test:  0.92899406\n",
      " Epoch : 83  Loss:  0.051197417 Test_Loss:  0.26424477\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.9112426\n",
      " Epoch : 84  Loss:  0.050049175 Test_Loss:  0.22823781\n",
      " Accuracy:  0.989726 Accuracy Test:  0.92899406\n",
      " Epoch : 85  Loss:  0.0486046 Test_Loss:  0.2642157\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.9112426\n",
      " Epoch : 86  Loss:  0.047494058 Test_Loss:  0.23089959\n",
      " Accuracy:  0.9914383 Accuracy Test:  0.92899406\n",
      " Epoch : 87  Loss:  0.046204247 Test_Loss:  0.26296705\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.91715974\n",
      " Epoch : 88  Loss:  0.045165088 Test_Loss:  0.23438512\n",
      " Accuracy:  0.9931507 Accuracy Test:  0.92899406\n",
      " Epoch : 89  Loss:  0.044013802 Test_Loss:  0.2619385\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.91715974\n",
      " Epoch : 90  Loss:  0.04305125 Test_Loss:  0.23740348\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.92899406\n",
      " Epoch : 91  Loss:  0.042002827 Test_Loss:  0.26050973\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.9230769\n",
      " Epoch : 92  Loss:  0.04113714 Test_Loss:  0.23945232\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.92899406\n",
      " Epoch : 93  Loss:  0.04019653 Test_Loss:  0.2613849\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.92899406\n",
      " Epoch : 94  Loss:  0.039352253 Test_Loss:  0.24141885\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.92899406\n",
      " Epoch : 95  Loss:  0.038498905 Test_Loss:  0.2618032\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.93491125\n",
      " Epoch : 96  Loss:  0.037704334 Test_Loss:  0.24243993\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.93491125\n",
      " Epoch : 97  Loss:  0.036883302 Test_Loss:  0.2621608\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.93491125\n",
      " Epoch : 98  Loss:  0.03621612 Test_Loss:  0.24245873\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.93491125\n",
      " Epoch : 99  Loss:  0.03543826 Test_Loss:  0.26505533\n",
      " Accuracy:  0.99486303 Accuracy Test:  0.93491125\n",
      "Elapsed time:  3.825865292549133 min\n"
     ]
    }
   ],
   "source": [
    "# Initialize placeholders\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, Taille_Sample, 3, 1], name=\"input\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"label\")\n",
    "step = tf.placeholder(dtype=tf.int32)\n",
    "\n",
    "# First convolution layer\n",
    "conv1 = tf.layers.conv2d(inputs= x,filters=16, kernel_size=4, padding=\"SAME\",strides=[1,1], activation=tf.nn.relu)\n",
    "conv2 = tf.layers.conv2d(inputs= conv1,filters=32, kernel_size=3, padding=\"SAME\",strides=[1,1], activation=tf.nn.relu)\n",
    "pool1 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=\"SAME\", name=\"pool1\")\n",
    "# Second convolution layer\n",
    "conv3 = tf.layers.conv2d(inputs= pool1,filters=64, kernel_size=4, padding=\"SAME\",strides=[1,1], activation=tf.nn.relu)\n",
    "conv4 = tf.layers.conv2d(inputs= conv3,filters=33, kernel_size=3, padding=\"SAME\",strides=[1,1], activation=tf.nn.relu)\n",
    "pool2 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding=\"SAME\", name=\"pool2\")\n",
    "\n",
    "shape = pool2.get_shape().as_list()\n",
    "reshaped = tf.reshape(pool2, shape=[-1, shape[1] * shape[2] * shape[3]])\n",
    "# First convolution layer\n",
    "dense1 = tf.contrib.layers.fully_connected(reshaped, 1000 ,activation_fn=tf.nn.relu,scope='Dense1')\n",
    "# Fully connected layer\n",
    "dense2 = tf.contrib.layers.fully_connected(dense1, 500 ,activation_fn=tf.nn.relu,scope='Dense2')\n",
    "dense3 = tf.contrib.layers.fully_connected(dense2, 100 ,activation_fn=tf.nn.relu,scope='Dense3')\n",
    "dense4 = tf.contrib.layers.fully_connected(dense3, 30 ,activation_fn=tf.nn.relu,scope='Dense4')\n",
    "# Final softmax layer\n",
    "logits = tf.contrib.layers.fully_connected(dense4, 4 ,activation_fn=None,scope='Dense5')\n",
    "softmax = tf.nn.softmax(logits, name = \"final_result\")  #normalise les valeurs dans l'intervalle [0, 1]\n",
    "\n",
    "#dense1 = add_dense_hidden_layer_op(x=reshaped, num_neurons_previous_layer=50*2, num_neurons_current_layer=200, name_scope=\"dense1\")\n",
    "#dense2 = add_dense_hidden_layer_op(x=dense1, num_neurons_previous_layer=200, num_neurons_current_layer=100, name_scope=\"dense2\")\n",
    "#dense3 = add_dense_hidden_layer_op(x=dense2, num_neurons_previous_layer=100, num_neurons_current_layer=60, name_scope=\"dense3\")\n",
    "#dense4 = add_dense_hidden_layer_op(x=dense3, num_neurons_previous_layer=60, num_neurons_current_layer=30, name_scope=\"dense4\")\n",
    "#softmax, logits = add_softmax_op(x=dense4, num_neurons_previous_layer=30, num_classes=2, name_scope=\"softmax\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    # Calcul de la fonction de coût\n",
    "    loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits (labels = y, \n",
    "                                                                    logits = logits,name=\"cross_entropy\"))\n",
    "    \n",
    "with tf.name_scope(\"Train\"):\n",
    "    # La fonction d'optimisation\n",
    "    lr = 0.0001 + tf.train.exponential_decay(0.01, step, 3000, 1/math.e) #\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate= lr).minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    # Trouve l'indice de la sortie la plus élevée, donc le label prédit\n",
    "    pred = tf.argmax(softmax, 1, name = \"out\")\n",
    "    # Calcul le taux de prédiction correct\n",
    "    correct_prediction = tf.equal(tf.cast(pred, tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"accuracy_value\")\n",
    " \n",
    "# L'opérateur qui permet de sauvegarder le modèle\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Ouverture de la Session TensorFlow\n",
    "sess = tf.Session()\n",
    "# Initialisation des variables du modèle\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# La boucle d'entrainement\n",
    "for i in range(100):\n",
    "    # On appel les opérateurs des différentes métriques\n",
    "    loss_val, accuracy_val= sess.run([loss, accuracy], feed_dict={x: features, y: labels})\n",
    "    \n",
    "    # On récupère le taux de prédiction sur les données test\n",
    "    test_accuracy_val,test_loss_val = sess.run([accuracy,loss], feed_dict={x: features_test, y: labels_test})\n",
    "    \n",
    "    # On appel l'opérateur d'entrainement\n",
    "    sess.run(train_op, feed_dict={x: features, y: labels, step: i})\n",
    "\n",
    "    print(\" Epoch :\", i,\" Loss: \", loss_val,\"Test_Loss: \", test_loss_val)\n",
    "    print(\" Accuracy: \", accuracy_val,\"Accuracy Test: \",test_accuracy_val)\n",
    "print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "meta_graph_def = tf.train.export_meta_graph(filename='./Model.meta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sterling/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/tools/freeze_graph.py:249: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "WARNING:tensorflow:From /home/sterling/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./running_model.ckpt\n",
      "WARNING:tensorflow:From /home/sterling/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/tools/freeze_graph.py:232: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/sterling/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 18 variables.\n",
      "INFO:tensorflow:Converted 18 variables to const ops.\n",
      "WARNING:tensorflow:From /home/sterling/.conda/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.remove_training_nodes\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde les variables du modèle sous forme de checkpoint (.ckpt)\n",
    "saver.save(sess, './running_model.ckpt')\n",
    "# Sauvegarde le graphe du modèle sous forme de Graph_Def\n",
    "tf.train.write_graph(sess.graph_def, \".\", 'running_model.pbtxt')\n",
    "\n",
    "# Ce sont les nom et adresse, et valeurs que l'on vas utiliser dans les script d'optimisations du modèle\n",
    "MODEL_NAME = 'running_model'\n",
    "input_graph_path = MODEL_NAME+'.pbtxt'\n",
    "checkpoint_path = './'+MODEL_NAME+'.ckpt'\n",
    "input_saver_def_path = \"\"\n",
    "input_binary = False\n",
    "output_node_names = \"final_result\"\n",
    "restore_op_name = \"save/restore_all\"\n",
    "filename_tensor_name = \"save/Const:0\"\n",
    "output_frozen_graph_name = 'frozen_'+MODEL_NAME+'.pb'\n",
    "output_optimized_graph_name = 'optimized_'+MODEL_NAME+'.pb'\n",
    "clear_devices = True\n",
    "\n",
    "# Permet de geler le graphe, \n",
    "freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n",
    "                         input_binary, checkpoint_path,output_node_names,\n",
    "                         restore_op_name,filename_tensor_name,\n",
    "                         output_frozen_graph_name,clear_devices,\"\")\n",
    "\n",
    "# Ouvre et charge le fichier du modèle gelé\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.Open(output_frozen_graph_name,\"rb\") as f:\n",
    "    data = f.read()\n",
    "    input_graph_def.ParseFromString(data)\n",
    "\n",
    "# Permet d'appliquer des optimisations au modèle\n",
    "output_graph_def = optimize_for_inference_lib.optimize_for_inference(input_graph_def,\n",
    "                                                                    [\"input\"],\n",
    "                                                                    [\"final_result\"],\n",
    "                                                                    tf.float32.as_datatype_enum)\n",
    "# On écrit et on sauvegarde les modifications dans un nouveau fichier\n",
    "f = tf.gfile.FastGFile(output_optimized_graph_name,\"w\")\n",
    "f.write(output_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = tf.placeholder(dtype=tf.float32, shape=[None,3], name=\"input\")\\ny = tf.placeholder(dtype=tf.int32, shape=[None], name=\"label\")\\n\\ndense1 = tf.contrib.layers.fully_connected(x, 4 ,activation_fn=tf.nn.relu,scope=\\'Dense1\\')\\ndense2 = tf.contrib.layers.fully_connected(dense1, 3 ,activation_fn=tf.nn.relu,scope=\\'Dense2\\')\\ndense3 = tf.contrib.layers.fully_connected(dense1, 4 ,activation_fn=tf.nn.softmax,scope=\\'Softmax\\')\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None,3], name=\"input\")\n",
    "y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"label\")\n",
    "\n",
    "dense1 = tf.contrib.layers.fully_connected(x, 4 ,activation_fn=tf.nn.relu,scope='Dense1')\n",
    "dense2 = tf.contrib.layers.fully_connected(dense1, 3 ,activation_fn=tf.nn.relu,scope='Dense2')\n",
    "dense3 = tf.contrib.layers.fully_connected(dense1, 4 ,activation_fn=tf.nn.softmax,scope='Softmax')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m Sample: 85 Label Actuel: 1.0 Label Prédit: 0 \u001b[0m\n",
      "\u001b[31m Sample: 92 Label Actuel: 1.0 Label Prédit: 2 \u001b[0m\n",
      "\u001b[31m Sample: 97 Label Actuel: 1.0 Label Prédit: 2 \u001b[0m\n",
      "\u001b[31m Sample: 103 Label Actuel: 1.0 Label Prédit: 2 \u001b[0m\n",
      "\u001b[31m Sample: 119 Label Actuel: 1.0 Label Prédit: 0 \u001b[0m\n",
      "\u001b[31m Sample: 123 Label Actuel: 1.0 Label Prédit: 0 \u001b[0m\n",
      "\u001b[31m Sample: 124 Label Actuel: 1.0 Label Prédit: 2 \u001b[0m\n",
      "\u001b[31m Sample: 139 Label Actuel: 2.0 Label Prédit: 1 \u001b[0m\n",
      "\u001b[31m Sample: 144 Label Actuel: 2.0 Label Prédit: 0 \u001b[0m\n",
      "\u001b[31m Sample: 151 Label Actuel: 2.0 Label Prédit: 0 \u001b[0m\n",
      "\u001b[31m Sample: 154 Label Actuel: 2.0 Label Prédit: 0 \u001b[0m\n",
      "Elapsed time:  1.0378031730651855 sec\n",
      "Nbr_predict_errors:  11\n",
      "labels_test.Shape:  (169,) loss: 6.508875739644971 % Win: 93.49112426035504 %\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "a=0\n",
    "for i in range (len(labels_test)):\n",
    "    # Affichage des tableaux qui récapitulent la variation des métriques durant l'entrainement\n",
    "    label_prédit, softmax_val = sess.run([pred, softmax], feed_dict={x: features_test[i].reshape(1,Taille_Sample,3,1)})\n",
    "    if (labels_test[i] != label_prédit):\n",
    "        print(\"\\x1b[31m Sample:\",i, \"Label Actuel:\", labels_test[i], \"Label Prédit:\", label_prédit[0], \"\\x1b[0m\")\n",
    "        a=a+1\n",
    "        #fig, axes = plt.subplots(1)\n",
    "\n",
    "        #axes.plot(features_test[i,:,:,0])\n",
    "        #plt.show()\n",
    "\n",
    "print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "print(\"Nbr_predict_errors: \",a)\n",
    "Pourcentage_loss = (a*100)/labels_test.shape[0]\n",
    "Pourcentage_win  = 100 - (a*100)/labels_test.shape[0]\n",
    "print(\"labels_test.Shape: \",labels_test.shape,\"loss:\",Pourcentage_loss,\"% Win:\",Pourcentage_win,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
